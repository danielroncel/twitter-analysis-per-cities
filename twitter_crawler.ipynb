{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most common words and bigrams in the last minutes per location in Spain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author: Daniel Roncel Díaz**\n",
    "\n",
    "**Date: 30/03/2021**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import tweepy\n",
    "import json\n",
    "import time\n",
    "import re # RegEx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import emoji\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load cities name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get the current path to access easily to input and output folders:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load all the Spain cities and its coordinates:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Abla', 'Abrucena', 'Adra', 'Albánchez', 'Alboloduy']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities_file = folder_path+'\\input\\spain_cities.csv'\n",
    "df_cities = pd.read_csv(cities_file)\n",
    "\n",
    "# cities_dict[city] = (latitude, longitude)\n",
    "cities_dict = dict()\n",
    "for idx, row in df_cities.iterrows():\n",
    "    cities_dict[row['town']] = (row['latitude'], row['longitude'])\n",
    "    \n",
    "# Show a few cities in the dictionary    \n",
    "list(cities_dict.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_filter(location, cities_dict, valid_end):\n",
    "    \"\"\"\n",
    "    Returns the city the location parameter is associated to.\n",
    "    If the city cannot be identified, returns and empty string.\n",
    "    \n",
    "    Arguments:\n",
    "        location:\n",
    "            string associated with the city that it is requested to find\n",
    "        cities_dict:\n",
    "            dictionary whose keys are the valid possible cities\n",
    "        valid_end:\n",
    "            set of possible valid suffixes (ignoring punctuation marks)\n",
    "    \"\"\"\n",
    "    if location in cities_dict:\n",
    "        coord = cities_dict[location]\n",
    "        return location\n",
    "    else:\n",
    "        # Split location by any character not included below\n",
    "        # Ensure that the accents are preserved when working with Spanish texts\n",
    "        location_split  = re.split(r'[^A-ZÁÀÉÈÍÌÏÓÒÚÜa-záàéèíìïóòúùü0-9]+', location)\n",
    "\n",
    "        # Remove all empty strings\n",
    "        location_split = [location_split[i] for i in range(len(location_split)) if location_split[i] != '']\n",
    "        \n",
    "        if len(location_split) <= 1:\n",
    "            return ''\n",
    "        elif location_split[0] in cities_dict:\n",
    "            if location_split[-1] in valid_end or location_split[-1] in cities_dict:\n",
    "                coord = cities_dict[location_split[0]]\n",
    "                return location_split[0]\n",
    "            else:\n",
    "                return ''\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load all the valid ends of a user location:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Andalucía',\n",
       " 'Aragón',\n",
       " 'Asturias',\n",
       " 'Canarias',\n",
       " 'Cantabria',\n",
       " 'Castilla y León',\n",
       " 'Castilla-La Mancha ',\n",
       " 'Catalunya',\n",
       " 'Cataluña',\n",
       " 'Ceuta',\n",
       " 'Comunidad Valenciana',\n",
       " 'Comunidad de Madrid',\n",
       " 'Comunitat Valenciana',\n",
       " 'España',\n",
       " 'Euskadi',\n",
       " 'Extremadura',\n",
       " 'Galicia',\n",
       " 'Galiza',\n",
       " 'Islas Baleares',\n",
       " 'Islas Canarias',\n",
       " 'La Rioja',\n",
       " 'Melilla',\n",
       " 'Navarra',\n",
       " 'País Vasco',\n",
       " 'Principado de Asturias',\n",
       " 'Región de Murcia',\n",
       " 'Spain'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_end = set()\n",
    "\n",
    "file_name = folder_path+\"/input/valid_ends.txt\"\n",
    "f = codecs.open(file_name, 'r', encoding='utf8')\n",
    "\n",
    "city = f.readline()\n",
    "while city != 'END':\n",
    "    valid_end.add(city[:-2])\n",
    "    city = f.readline()\n",
    "f.close()\n",
    "\n",
    "valid_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Madrid'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "location = \"Madrid, Spain\"\n",
    "location_filter(location, cities_dict, valid_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set up our crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Listener(tweepy.streaming.StreamListener):\n",
    "    \"\"\"\n",
    "    Twitter crawler for tweet streaming.\n",
    "    \"\"\"\n",
    "    def __init__(self, cities_dict, valid_end, cities_filter_function, max_minutes, \\\n",
    "                                             json_tweets_file, max_tweets=None, api=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            cities_dict:\n",
    "                dictionary whose keys are the valid cities\n",
    "                from which we can get tweets.\n",
    "            valid_end:\n",
    "                set of possible valid suffixes (ignoring punctuation marks).\n",
    "            cities_filter_function:\n",
    "                returns the city the location parameter is associated to. \n",
    "                If the location cannot be processed or it is not valid, \n",
    "                returns an empty string.\n",
    "            max_minutes:\n",
    "                maximum number of minutes streaming tweets.\n",
    "            max_tweets:\n",
    "                maximum number of tweets to stream.\n",
    "            json_tweets_file:\n",
    "                file to store the streamed tweets.\n",
    "            api:\n",
    "                attribute of the parent class. It won't be used in this code.\n",
    "        \"\"\"\n",
    "        super(tweepy.streaming.StreamListener, self).__init__()\n",
    "        self.cities_dict = cities_dict\n",
    "        self.valid_end = valid_end\n",
    "        self.cities_filter_function = cities_filter_function\n",
    "        self.num_tweets = 0 # Current read tweets\n",
    "        self.max_tweets = max_tweets\n",
    "        self.time_limit = time.time() + max_minutes*60\n",
    "        self.json_tweets_file = json_tweets_file\n",
    "        self.api = api\n",
    "        \n",
    "    def on_data(self, data):\n",
    "        \"\"\"\n",
    "        Function called every time a tweet is streamed. If the user's location is valid\n",
    "        and can be retrieved by self.cities_filter_function, stores the tweet with its city \n",
    "        in json format in self.json_tweets_file.\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Append the new tweet in the file, if location is retrived\n",
    "            with open(self.json_tweets_file, 'a') as f:\n",
    "                tweet_json = json.loads(data)\n",
    "                location = tweet_json['user']['location']\n",
    "                \n",
    "                if location != None:\n",
    "                    # Try to retrieve user's location\n",
    "                    user_location = self.cities_filter_function(location, self.cities_dict, self.valid_end)\n",
    "                    # If it could be retrieved\n",
    "                    if user_location != '':\n",
    "                        tweet_json['user_location'] = user_location\n",
    "                        f.write(json.dumps(tweet_json) + \"\\n\\n\")\n",
    "                        \n",
    "                        # Print the retrieved location. Useful for testing cities_filter_function\n",
    "                        print(location +\" -> \"+user_location)\n",
    "                        \n",
    "                        self.num_tweets += 1\n",
    "                        # If we read enough tweets our streamed for enough time, stop the exection\n",
    "                        if self.max_tweets != None and self.num_tweets >= self.max_tweets:\n",
    "                            return False\n",
    "                        if self.time_limit < time.time():\n",
    "                            return False\n",
    "                        return True\n",
    "                return True\n",
    "        # If an error occurs, stop the execution\n",
    "        except Exception as e:\n",
    "            print(type(e).__name__)\n",
    "            return True\n",
    "        \n",
    "    def on_error(self, status):\n",
    "        print('Error :', status)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill your twitter developer keys and secrets\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_secret = ''\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esparreguera -> Esparreguera\n",
      "Aranjuez -> Aranjuez\n",
      "Madrid - Valencia -> Madrid\n",
      "Alcobendas.Madrid -> Alcobendas\n",
      "Madrid -> Madrid\n",
      "Madrid -> Madrid\n",
      "Avilés -> Avilés\n",
      "Madrid -> Madrid\n",
      "Cádiz -> Cádiz\n",
      "Madrid (Spain) -> Madrid\n",
      "Madrid, Comunidad de Madrid -> Madrid\n",
      "Écija, Sevilla -> Écija\n",
      "Madrid -> Madrid\n",
      "Madrid -> Madrid\n",
      "Valencia -> Valencia\n",
      "Barcelona -> Barcelona\n",
      "Madrid -> Madrid\n",
      "Valencia -> Valencia\n",
      "Madrid -> Madrid\n",
      "Madrid -> Madrid\n",
      "Leganés -> Leganés\n",
      "Madrid -> Madrid\n",
      "Madrid -> Madrid\n",
      "Algeciras / Granada -> Algeciras\n",
      "Madrid -> Madrid\n",
      "Madrid - Azeroth - Bilbao  -> Madrid\n",
      "Sevilla. Natural de Sevilla -> Sevilla\n",
      "Madrid -> Madrid\n",
      "Madrid -> Madrid\n",
      "Madrid -> Madrid\n",
      "Madrid, Comunidad de Madrid -> Madrid\n",
      "Utebo (Zaragoza) -> Utebo\n",
      "Collado Villalba, Madrid -> Collado\n",
      "Madrid -> Madrid\n",
      "Zaragoza/ Madrid/ Barcelona / -> Zaragoza\n",
      "Barcelona y Sitges -> Barcelona\n",
      "Chiva, Valencia -> Chiva\n",
      "Madrid -> Madrid\n",
      "Madrid, Comunidad de Madrid -> Madrid\n",
      "Avilés -> Avilés\n",
      "Madrid, Comunidad de Madrid -> Madrid\n",
      "Santa Cruz de Tenerife -> Santa Cruz de Tenerife\n",
      "Madrid -> Madrid\n",
      "Ourense -> Ourense\n",
      "Potes -> Potes\n",
      "_______ End _______\n"
     ]
    }
   ],
   "source": [
    "# Set your keywords for the search\n",
    "keywords = [\"madrid\"]\n",
    "# New folder name: keyword1_keyword2_..._keywordN_DAY-MONTH-YEAR_HOUR.MINUTE.SECOND.jsonl\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d-%m-%Y_%H.%M.%S\")\n",
    "output_folder = folder_path+'/output/'+'_'.join(keywords)+'_'+dt_string\n",
    "os.mkdir(output_folder)\n",
    "# File name: keyword1_keyword2_..._keywordN.jsonl\n",
    "# It will contain the tweets in json format\n",
    "json_tweets_file = output_folder+'/'+'_'.join(keywords)+'.jsonl'\n",
    "\n",
    "# Execution time\n",
    "max_minutes = 1\n",
    "twitter_stream = tweepy.streaming.Stream(auth, Listener(cities_dict=cities_dict, valid_end=valid_end, cities_filter_function=location_filter, max_minutes=max_minutes, json_tweets_file = json_tweets_file))\n",
    "# Filter for the given keywords in Spanish tweets\n",
    "twitter_stream.filter(track=keywords, languages=['es']) \n",
    "\n",
    "print('_______ End _______')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process the stored data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the stored tweets in a pandas Dataframe:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contributors</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>display_text_range</th>\n",
       "      <th>entities</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>extended_tweet</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>filter_level</th>\n",
       "      <th>...</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>retweeted_status</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp_ms</th>\n",
       "      <th>truncated</th>\n",
       "      <th>user</th>\n",
       "      <th>user_location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-03-30 12:33:26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'hashtags': [], 'urls': [], 'user_mentions': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'created_at': 'Tue Mar 30 12:09:44 +0000 2021...</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>RT @marianpy1: No tienen vergüenza, esto deber...</td>\n",
       "      <td>2021-03-30 12:33:26.167</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 636170404, 'id_str': '636170404', 'name...</td>\n",
       "      <td>Esparreguera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-03-30 12:33:27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'hashtags': [], 'urls': [], 'user_mentions': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'created_at': 'Tue Mar 30 12:29:57 +0000 2021...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>RT @pradobenjamin: Ayuso pone barra libre en M...</td>\n",
       "      <td>2021-03-30 12:33:27.427</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 375801242, 'id_str': '375801242', 'name...</td>\n",
       "      <td>Aranjuez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-03-30 12:33:29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'hashtags': [], 'urls': [{'url': 'https://t.c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>DEL AEROPUERTO DE BARAJAS AL CENTRO DE MADRID ...</td>\n",
       "      <td>2021-03-30 12:33:29.931</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 541618181, 'id_str': '541618181', 'name...</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-03-30 12:33:32</td>\n",
       "      <td>[15, 62]</td>\n",
       "      <td>{'hashtags': [{'text': 'SiSePuede', 'indices':...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>@PabloIglesias Gracias por tu esfuerzo.A por M...</td>\n",
       "      <td>2021-03-30 12:33:32.446</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 1876488498, 'id_str': '1876488498', 'na...</td>\n",
       "      <td>Alcobendas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-03-30 12:33:33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'hashtags': [], 'urls': [], 'user_mentions': ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'created_at': 'Tue Mar 30 12:32:29 +0000 2021...</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>RT @Rita_Maestre: Día histórico para Vallecas ...</td>\n",
       "      <td>2021-03-30 12:33:33.362</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 338560854, 'id_str': '338560854', 'name...</td>\n",
       "      <td>Madrid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   contributors  coordinates          created_at display_text_range  \\\n",
       "0           NaN          NaN 2021-03-30 12:33:26                NaN   \n",
       "1           NaN          NaN 2021-03-30 12:33:27                NaN   \n",
       "2           NaN          NaN 2021-03-30 12:33:29                NaN   \n",
       "3           NaN          NaN 2021-03-30 12:33:32           [15, 62]   \n",
       "4           NaN          NaN 2021-03-30 12:33:33                NaN   \n",
       "\n",
       "                                            entities extended_entities  \\\n",
       "0  {'hashtags': [], 'urls': [], 'user_mentions': ...               NaN   \n",
       "1  {'hashtags': [], 'urls': [], 'user_mentions': ...               NaN   \n",
       "2  {'hashtags': [], 'urls': [{'url': 'https://t.c...               NaN   \n",
       "3  {'hashtags': [{'text': 'SiSePuede', 'indices':...               NaN   \n",
       "4  {'hashtags': [], 'urls': [], 'user_mentions': ...               NaN   \n",
       "\n",
       "  extended_tweet  favorite_count  favorited filter_level      ...       \\\n",
       "0            NaN               0      False          low      ...        \n",
       "1            NaN               0      False          low      ...        \n",
       "2            NaN               0      False          low      ...        \n",
       "3            NaN               0      False          low      ...        \n",
       "4            NaN               0      False          low      ...        \n",
       "\n",
       "   reply_count  retweet_count  retweeted  \\\n",
       "0            0              0      False   \n",
       "1            0              0      False   \n",
       "2            0              0      False   \n",
       "3            0              0      False   \n",
       "4            0              0      False   \n",
       "\n",
       "                                    retweeted_status  \\\n",
       "0  {'created_at': 'Tue Mar 30 12:09:44 +0000 2021...   \n",
       "1  {'created_at': 'Tue Mar 30 12:29:57 +0000 2021...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  {'created_at': 'Tue Mar 30 12:32:29 +0000 2021...   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "1  <a href=\"http://twitter.com/download/android\" ...   \n",
       "2  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "3  <a href=\"http://twitter.com/download/android\" ...   \n",
       "4  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "\n",
       "                                                text            timestamp_ms  \\\n",
       "0  RT @marianpy1: No tienen vergüenza, esto deber... 2021-03-30 12:33:26.167   \n",
       "1  RT @pradobenjamin: Ayuso pone barra libre en M... 2021-03-30 12:33:27.427   \n",
       "2  DEL AEROPUERTO DE BARAJAS AL CENTRO DE MADRID ... 2021-03-30 12:33:29.931   \n",
       "3  @PabloIglesias Gracias por tu esfuerzo.A por M... 2021-03-30 12:33:32.446   \n",
       "4  RT @Rita_Maestre: Día histórico para Vallecas ... 2021-03-30 12:33:33.362   \n",
       "\n",
       "   truncated                                               user user_location  \n",
       "0      False  {'id': 636170404, 'id_str': '636170404', 'name...  Esparreguera  \n",
       "1      False  {'id': 375801242, 'id_str': '375801242', 'name...      Aranjuez  \n",
       "2      False  {'id': 541618181, 'id_str': '541618181', 'name...        Madrid  \n",
       "3      False  {'id': 1876488498, 'id_str': '1876488498', 'na...    Alcobendas  \n",
       "4      False  {'id': 338560854, 'id_str': '338560854', 'name...        Madrid  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(json_tweets_file, lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Normalize the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    \"\"\"\n",
    "    Remove all the emojis, accents, undesired characters\n",
    "    in a string and convert it to lowercase.\n",
    "    \n",
    "    Arguments:\n",
    "        text:\n",
    "            string to normalize.\n",
    "    \"\"\"\n",
    "    # Delete emojis\n",
    "    text = re.sub(emoji.get_emoji_regexp(), r' ', text)\n",
    "    # Delete accents. With this approach we can keep the 'ñ' char, common in Spanish\n",
    "    text = text.lower()\n",
    "    original_char = 'ÁÀÉÈÍÌÏÓÒÚÙÜáàéèíìïóòúùü'\n",
    "    new_char = 'AAEEIIIOOUUUaaeeiioouuu'\n",
    "    \n",
    "    for original, new in zip(original_char, new_char):\n",
    "        text = text.replace(original, new)\n",
    "    \n",
    "    # Delete all the chars not included below. Transform it to lowercase\n",
    "    text = re.sub(r'[^A-ZÑa-zñ0-9_ ]+', ' ', text).lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**See some of the original tweets to compare them with the normalized ones:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    RT @marianpy1: No tienen vergüenza, esto deber...\n",
       "1    RT @pradobenjamin: Ayuso pone barra libre en M...\n",
       "2    DEL AEROPUERTO DE BARAJAS AL CENTRO DE MADRID ...\n",
       "3    @PabloIglesias Gracias por tu esfuerzo.A por M...\n",
       "4    RT @Rita_Maestre: Día histórico para Vallecas ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize the tweets:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    rt  marianpy1  no tienen verg enza  esto deber...\n",
       "1    rt  pradobenjamin  ayuso pone barra libre en m...\n",
       "2    del aeropuerto de barajas al centro de madrid ...\n",
       "3     pabloiglesias gracias por tu esfuerzo a por m...\n",
       "4    rt  rita_maestre  dia historico para vallecas ...\n",
       "Name: text_normalized, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_normalized'] = df['text'].apply(normalize)\n",
    "df['text_normalized'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Compute the list of words of each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_words(text, stopwords_set, min_size):\n",
    "    \"\"\"\n",
    "    Return all the words in a string that aren't stopwords and\n",
    "    that are at least as large as the specified size.\n",
    "    \n",
    "    Arguments:\n",
    "        text:\n",
    "            string whose words we are interested in.\n",
    "        stopwords_set:\n",
    "            set of stopwords.\n",
    "        min_size:\n",
    "            minimum size of a word to be returned.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word != '' and len(word) >= MIN_SIZE and word not in stopwords_set]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [marianpy1, verg, enza, deberia, ser, delito, ...\n",
       "1    [pradobenjamin, ayuso, pone, barra, libre, mad...\n",
       "2    [aeropuerto, barajas, centro, madrid, j0hvw8ss...\n",
       "3    [pabloiglesias, gracias, esfuerzo, madrid, sis...\n",
       "4    [rita_maestre, dia, historico, vallecas, madri...\n",
       "Name: text_words, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stopwords are the words that we want to delete from all tweets\n",
    "STOPWORDS = set(stopwords.words('spanish') + [\"rt\", \"http\", \"https\", \"co\", \"com\", \"es\"])\n",
    "# Minimum size of a word to be stored\n",
    "MIN_SIZE = 2\n",
    "\n",
    "df['text_words'] = df['text_normalized'].apply(lambda text: text_to_words(text, STOPWORDS, MIN_SIZE))\n",
    "df['text_words'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Compute the list of bigrams of each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(marianpy1, verg), (verg, enza), (enza, deber...\n",
       "1    [(pradobenjamin, ayuso), (ayuso, pone), (pone,...\n",
       "2    [(aeropuerto, barajas), (barajas, centro), (ce...\n",
       "3    [(pabloiglesias, gracias), (gracias, esfuerzo)...\n",
       "4    [(rita_maestre, dia), (dia, historico), (histo...\n",
       "Name: text_bigrams, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_bigrams'] = df['text_words'].apply(lambda text: list(ngrams(text,2)))\n",
    "df['text_bigrams'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Compute the top used words per location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_words</th>\n",
       "      <th>text_bigrams</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_location</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alcobendas</th>\n",
       "      <td>[pabloiglesias, gracias, esfuerzo, madrid, sis...</td>\n",
       "      <td>[(pabloiglesias, gracias), (gracias, esfuerzo)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algeciras</th>\n",
       "      <td>[noelia_n, magic, english, spain, means, españ...</td>\n",
       "      <td>[(noelia_n, magic), (magic, english), (english...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aranjuez</th>\n",
       "      <td>[pradobenjamin, ayuso, pone, barra, libre, mad...</td>\n",
       "      <td>[(pradobenjamin, ayuso), (ayuso, pone), (pone,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avilés</th>\n",
       "      <td>[policia, detenidos, seis, jovenes, banda, lat...</td>\n",
       "      <td>[(policia, detenidos), (detenidos, seis), (sei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Barcelona</th>\n",
       "      <td>[miirerme, ojala, tener, autoestima, madrileño...</td>\n",
       "      <td>[(miirerme, ojala), (ojala, tener), (tener, au...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text_words  \\\n",
       "user_location                                                      \n",
       "Alcobendas     [pabloiglesias, gracias, esfuerzo, madrid, sis...   \n",
       "Algeciras      [noelia_n, magic, english, spain, means, españ...   \n",
       "Aranjuez       [pradobenjamin, ayuso, pone, barra, libre, mad...   \n",
       "Avilés         [policia, detenidos, seis, jovenes, banda, lat...   \n",
       "Barcelona      [miirerme, ojala, tener, autoestima, madrileño...   \n",
       "\n",
       "                                                    text_bigrams  \n",
       "user_location                                                     \n",
       "Alcobendas     [(pabloiglesias, gracias), (gracias, esfuerzo)...  \n",
       "Algeciras      [(noelia_n, magic), (magic, english), (english...  \n",
       "Aranjuez       [(pradobenjamin, ayuso), (ayuso, pone), (pone,...  \n",
       "Avilés         [(policia, detenidos), (detenidos, seis), (sei...  \n",
       "Barcelona      [(miirerme, ojala), (ojala, tener), (tener, au...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wb = df[['text_words', 'text_bigrams', 'user_location']]\n",
    "df_wb = df_wb.groupby('user_location').sum()\n",
    "df_wb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_location\n",
       "Alcobendas    [(pabloiglesias, 1), (gracias, 1), (esfuerzo, ...\n",
       "Algeciras     [(noelia_n, 1), (magic, 1), (english, 1), (spa...\n",
       "Aranjuez      [(pradobenjamin, 1), (ayuso, 1), (pone, 1), (b...\n",
       "Avilés        [(policia, 1), (detenidos, 1), (seis, 1), (jov...\n",
       "Barcelona     [(navarra, 2), (miirerme, 1), (ojala, 1), (ten...\n",
       "Name: top_words, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of words to be shown per city\n",
    "TOP = 5\n",
    "# Compute the TOP words more used per city\n",
    "df_wb['top_words'] = df_wb['text_words'].apply(lambda words: Counter(words).most_common(TOP))\n",
    "df_wb['top_words'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Compute the top used bigrams per location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_location\n",
       "Alcobendas    [((pabloiglesias, gracias), 1), ((gracias, esf...\n",
       "Algeciras     [((noelia_n, magic), 1), ((magic, english), 1)...\n",
       "Aranjuez      [((pradobenjamin, ayuso), 1), ((ayuso, pone), ...\n",
       "Avilés        [((policia, detenidos), 1), ((detenidos, seis)...\n",
       "Barcelona     [((miirerme, ojala), 1), ((ojala, tener), 1), ...\n",
       "Name: text_bigrams, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOP = 5\n",
    "df_wb['text_bigrams'] = df_wb['text_bigrams'].apply(lambda bigrams: Counter(bigrams).most_common(TOP))\n",
    "df_wb['text_bigrams'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Compute general stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1 Counter of tweets per city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Madrid       24\n",
       "Avilés        2\n",
       "Barcelona     2\n",
       "Valencia      2\n",
       "Sevilla       1\n",
       "Name: user_location, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_per_location = df['user_location'].value_counts()\n",
    "users_per_location.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2 Top used words without regarding the location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_freq = dict()\n",
    "\n",
    "for location, row in df_wb.iterrows():\n",
    "    for word, ctr in row['top_words']:\n",
    "        if word not in words_freq:\n",
    "            words_freq[word] = ctr\n",
    "        else:\n",
    "            words_freq[word] += ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('madrid', 15), ('dia', 6), ('30', 3), ('gobierno', 3), ('turistas', 3), ('va', 3), ('navarra', 2), ('cuesta', 2), ('ser', 2), ('paco', 2)]\n"
     ]
    }
   ],
   "source": [
    "TOP = 10\n",
    "words_freq = Counter(words_freq).most_common(TOP)\n",
    "print(words_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.3 Top used bigrams without regarding the location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_freq = dict()\n",
    "\n",
    "for location, row in df_wb.iterrows():\n",
    "    for bigrams, ctr in row['text_bigrams']:\n",
    "        if bigrams not in bigrams_freq:\n",
    "            bigrams_freq[bigrams] = ctr\n",
    "        else:\n",
    "            bigrams_freq[bigrams] += ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('rita_maestre', 'dia'), 2), (('dia', 'historico'), 2), (('historico', 'vallecas'), 2), (('vallecas', 'madrid'), 2), (('madrid', 'ganamos'), 2), (('va', 'dejar'), 2), (('dejar', 'pagar'), 2), (('pabloiglesias', 'gracias'), 1), (('gracias', 'esfuerzo'), 1), (('esfuerzo', 'madrid'), 1)]\n"
     ]
    }
   ],
   "source": [
    "TOP = 10\n",
    "bigrams_freq = Counter(bigrams_freq).most_common(TOP)\n",
    "print(bigrams_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Save the results in files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Store the top words used per location:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_file = output_folder+'/words_location.txt'\n",
    "\n",
    "with open(words_file, 'w') as f:\n",
    "    for location, row in df_wb.iterrows():\n",
    "        f.write(location+'\\n')\n",
    "        \n",
    "        for word, ctr in row['top_words']:\n",
    "            f.write(word+' '+str(ctr)+'\\n')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Store the top bigrams used per location:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_file = output_folder+'/bigrams_location.txt'\n",
    "\n",
    "with open(bigrams_file, 'w') as f:\n",
    "    for location, row in df_wb.iterrows():\n",
    "        f.write(location +'\\n')\n",
    "        \n",
    "        for bigram, ctr in row['text_bigrams']:\n",
    "            f.write(bigram[0]+' '+bigram[1]+' '+str(ctr)+'\\n')\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Store the number of tweets per location:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_file = output_folder+'/counter_location.txt'\n",
    "\n",
    "with open(main_file, 'w') as f:\n",
    "    for location in users_per_location.index:\n",
    "        f.write(location+' '+str(users_per_location[location])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Store the top used words without regarding the location:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_file = output_folder+'/words.txt'\n",
    "\n",
    "with open(words_file, 'w') as f:\n",
    "    for word, ctr in words_freq:\n",
    "        f.write(word+' '+str(ctr)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Store the global top used bigrams without regarding the location:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_file = output_folder+'/bigrams.txt'\n",
    "\n",
    "with open(bigrams_file, 'w') as f:\n",
    "    for bigram, ctr in bigrams_freq:\n",
    "        f.write(bigram[0]+' '+bigram[1]+' '+str(ctr)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
